{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![header image](https://github.com/Lexie88rus/quick-draw-image-recognition/blob/master/app/static/jumbotron.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Deep Learning and PyTorch for Quick, Draw! Doodles Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINITION\n",
    "\n",
    "### Overview\n",
    "The [Quick Draw](https://github.com/googlecreativelab/quickdraw-dataset) Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game [Quick, Draw!](https://quickdraw.withgoogle.com/). The player starts with an object to draw (for example it may say \"Draw a chair in under 20 seconds\"). Then the player has twenty seconds to draw that object. Based on what they draw, the AI guesses what they are drawing.\n",
    "Research in recognition of images drawn by humans can improve pattern recognition solutions more broadly. Improving pattern recognition has an impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing).\n",
    "__In this kernel I analyzed the drawings and tried to build a deep learning application to classify those drawings__ (in this [GitHub repository](https://github.com/Lexie88rus/quick-draw-image-recognition) you can find the code for the resulting web application to play around with the model).\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Recognition of a drawing is a classification problem. I have to build a solution, which classifies input images. I split the whole problem of recognition of drawings into the following tasks:\n",
    "* Input data analysis and preprocessing;\n",
    "* Building a model to classify drawings;\n",
    "* Evaluation of the model concerning chosen metrics;\n",
    "* Building a web-application to demonstrate the results.\n",
    "\n",
    "I am new to deep learning, so I simplified this task to only ten classes from the dataset. I will also shrink the input images to 28x28 pixels in order to be able to use simple fully connected network to classify the images.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "I chose accuracy as a metric to evaluate the results. Because of the rules of the game, we mostly care about how many times did the AI recognize the drawing correctly, and this is just the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Import helper libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import requests\n",
    "from io import BytesIO # Use When expecting bytes-like objects\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from os import path\n",
    "import ast\n",
    "import random\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "import json\n",
    "import tarfile\n",
    "\n",
    "# import matplotlib for visualization\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import PIL for image manipulation\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# import boto3\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "# import kubeflow metadata\n",
    "from kubeflow.metadata import metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to load the simplified data for 10 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp gs://artifacts.kubeflow-aifest-2019.appspot.com/quickdraw_dataset/*.pickle ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 10 classes to load the data for\n",
    "categories = ['cannon','eye', 'face', 'nail', 'pear','piano','radio','spider','star','sword']\n",
    "label_dict = {0:'cannon',1:'eye', 2:'face', 3:'nail', 4:'pear',\n",
    "              5:'piano',6:'radio', 7:'spider', 8:'star', 9:'sword'}\n",
    "\n",
    "# load data for each category\n",
    "classes = {}\n",
    "for category in categories:\n",
    "    data = pd.read_pickle(category + '.pickle')\n",
    "    classes[category] = data\n",
    "    print(\"Loaded \" + category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to work with a simplified representation of images. I will shrink initial images to 28x28 grayscale images. For image manipulation I am going to use utility functions described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Image manipulation utilities: \n",
    "\n",
    "def convert_to_PIL(drawing, width = 256, height = 256):\n",
    "    \"\"\"\n",
    "    Function to convert from drawing to PIL image.\n",
    "    INPUT:\n",
    "        drawing - drawing from 'drawing' column\n",
    "        width - width of the initial image\n",
    "        height - height of the initial image\n",
    "    OUTPUT:\n",
    "        pil_img - (PIL Image) image\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize empty (white) PIL image\n",
    "    pil_img = Image.new('RGB', (width, height), 'white')\n",
    "    pixels = pil_img.load()\n",
    "            \n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    \n",
    "    # draw strokes as lines\n",
    "    for x,y in drawing:\n",
    "        for i in range(1, len(x)):\n",
    "            draw.line((x[i-1], y[i-1], x[i], y[i]), fill=0)\n",
    "        \n",
    "    return pil_img\n",
    "\n",
    "def convert_to_np_raw(drawing, width = 256, height = 256):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        drawing - drawing in initial format\n",
    "        width - width of the initial image\n",
    "        height - height of the initial image\n",
    "    OUTPUT:\n",
    "        img - drawing converted to the numpy array (28 X 28)\n",
    "    \"\"\"\n",
    "    # initialize empty numpy array\n",
    "    img = np.zeros((28, 28))\n",
    "    \n",
    "    # create a PIL image out of drawing\n",
    "    pil_img = convert_to_PIL(drawing)\n",
    "    \n",
    "    #resize to 28,28\n",
    "    pil_img.thumbnail((28,28), Image.ANTIALIAS)\n",
    "    \n",
    "    pil_img = pil_img.convert('RGB')\n",
    "    pixels = pil_img.load()\n",
    "    \n",
    "    # fill in numpy array with pixel values\n",
    "    for i in range(0, 28):\n",
    "        for j in range(0, 28):\n",
    "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
    "    \n",
    "    return img\n",
    "\n",
    "def convert_to_np(pil_img, width = 256, height = 256):\n",
    "    \"\"\"\n",
    "    Function to convert PIL Image to numpy array.\n",
    "    INPUT:\n",
    "        pil_img - (PIL Image) image to be converted\n",
    "    OUTPUT:\n",
    "        img - (numpy array) converted image with shape (width, height)\n",
    "    \"\"\"\n",
    "    pil_img = pil_img.convert('RGB')\n",
    "\n",
    "    img = np.zeros((width, height))\n",
    "    pixels = pil_img.load()\n",
    "\n",
    "    for i in range(0, width):\n",
    "        for j in range(0, height):\n",
    "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
    "\n",
    "    return img\n",
    "\n",
    "def view_image(img, width = 256, height = 256):\n",
    "    \"\"\"\n",
    "    Function to view numpy image with matplotlib.\n",
    "    The function saves the image as png.\n",
    "    INPUT:\n",
    "        img - (numpy array) image from train dataset with size (1, 784)\n",
    "    OUTPUT:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6,9))\n",
    "    ax.imshow(img.reshape(width, height).squeeze())\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def crop_image(image):\n",
    "    \"\"\"\n",
    "    Crops image (crops out white spaces).\n",
    "    INPUT:\n",
    "        image - PIL image of original size to be cropped\n",
    "    OUTPUT:\n",
    "        cropped_image - PIL image cropped to the center  and resized to (28, 28)\n",
    "    \"\"\"\n",
    "    cropped_image = image\n",
    "\n",
    "    # get image size\n",
    "    width, height = cropped_image.size\n",
    "\n",
    "    # get image pixels\n",
    "    pixels = cropped_image.load()\n",
    "\n",
    "    image_strokes_rows = []\n",
    "    image_strokes_cols = []\n",
    "\n",
    "    # run through the image\n",
    "    for i in range(0, width):\n",
    "        for j in range(0, height):\n",
    "            # save coordinates of the image\n",
    "            if (pixels[i,j][0] > 0):\n",
    "                image_strokes_cols.append(i)\n",
    "                image_strokes_rows.append(j)\n",
    "\n",
    "    # if image is not empty then crop to contents of the image\n",
    "    if (len(image_strokes_rows)) > 0:\n",
    "        # find the box for image\n",
    "        row_min = np.array(image_strokes_rows).min()\n",
    "        row_max = np.array(image_strokes_rows).max()\n",
    "        col_min = np.array(image_strokes_cols).min()\n",
    "        col_max = np.array(image_strokes_cols).max()\n",
    "\n",
    "        # find the box for cropping\n",
    "        margin = min(row_min, height - row_max, col_min, width - col_max)\n",
    "\n",
    "        # crop image\n",
    "        border = (col_min, row_min, width - col_max, height - row_max)\n",
    "        cropped_image = ImageOps.crop(cropped_image, border)\n",
    "\n",
    "    # get cropped image size\n",
    "    width_cropped, height_cropped = cropped_image.size\n",
    "\n",
    "    # create square resulting image to paste cropped image into the center\n",
    "    dst_im = Image.new(\"RGBA\", (max(width_cropped, height_cropped), max(width_cropped, height_cropped)), \"white\")\n",
    "    offset = ((max(width_cropped, height_cropped) - width_cropped) // 2, (max(width_cropped, height_cropped) - height_cropped) // 2)\n",
    "    # paste to the center of a resulting image\n",
    "    dst_im.paste(cropped_image, offset)\n",
    "\n",
    "    #resize to 28,28\n",
    "    dst_im.thumbnail((28,28), Image.ANTIALIAS)\n",
    "\n",
    "    return dst_im\n",
    "\n",
    "def normalize(arr):\n",
    "    \"\"\"\n",
    "    Function performs the linear normalizarion of the array.\n",
    "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
    "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
    "    INPUT:\n",
    "        arr - orginal numpy array\n",
    "    OUTPUT:\n",
    "        arr - normalized numpy array\n",
    "    \"\"\"\n",
    "    arr = arr.astype('float')\n",
    "    # Do not touch the alpha channel\n",
    "    for i in range(3):\n",
    "        minval = arr[...,i].min()\n",
    "        maxval = arr[...,i].max()\n",
    "        if minval != maxval:\n",
    "            arr[...,i] -= minval\n",
    "            arr[...,i] *= (255.0/(maxval-minval))\n",
    "    return arr\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Function performs the normalization of the image.\n",
    "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
    "    INPUT:\n",
    "        image - PIL image to be normalized\n",
    "    OUTPUT:\n",
    "        new_img - PIL image normalized\n",
    "    \"\"\"\n",
    "    arr = np.array(image)\n",
    "    new_img = Image.fromarray(normalize(arr).astype('uint8'),'RGBA')\n",
    "    return new_img\n",
    "\n",
    "def rotate_image(src_im, angle = 45, size = (28,28)):\n",
    "    \"\"\"\n",
    "    Function to rotate PIL Image file\n",
    "    INPUT:\n",
    "        src_im - (PIL Image) 28x28 image to be rotated\n",
    "        angle - angle to rotate the image\n",
    "        size - (tuple) size of the output image\n",
    "    OUTPUT:\n",
    "        dst_im - (PIL Image) rotated image\n",
    "    \"\"\"\n",
    "    dst_im = Image.new(\"RGBA\", size, \"white\")\n",
    "    src_im = src_im.convert('RGBA')\n",
    "\n",
    "    rot = src_im.rotate(angle)\n",
    "    dst_im.paste(rot, (0, 0), rot)\n",
    "\n",
    "    return dst_im\n",
    "\n",
    "def flip_image(src_im):\n",
    "    \"\"\"\n",
    "    Function to flip a PIL Image file.\n",
    "    INPUT:\n",
    "        scr_im - (PIL Image) image to be flipped\n",
    "    OUTPUT:\n",
    "        dst_im - (PIL Image) flipped image\n",
    "    \"\"\"\n",
    "    dst_im = src_im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return dst_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shrink the images and create datasets with images and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shrinking the images\n",
    "\n",
    "# create the dictionary containing classes names as keys and images as values\n",
    "values_dict = {}\n",
    "for category in categories:\n",
    "    data = classes[category][:3000]\n",
    "    values = [convert_to_np_raw(img).reshape(1, 784) for img in data['drawing'].values]\n",
    "    values_dict[category] = values\n",
    "    \n",
    "# concatenate to create X (values) and y (labels) datasets\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for key, value in label_dict.items():\n",
    "    data_i = values_dict[value]\n",
    "    Xi = np.concatenate(data_i, axis = 0)\n",
    "    yi = np.full((len(Xi), 1), key).ravel()\n",
    "    \n",
    "    X.append(Xi)\n",
    "    y.append(yi)\n",
    "    \n",
    "X = np.concatenate(X, axis = 0)\n",
    "y = np.concatenate(y, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview some random examples of the images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def view_images_grid(X, y):\n",
    "    \"\"\"\n",
    "    Function to plot grid with several examples of images.\n",
    "    INPUT:\n",
    "        X - (numpy array) images dataset\n",
    "        y - (numpy array) labels for images from X dataset\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(5, 10, figsize=(20,10))\n",
    "    \n",
    "    for label_num in range(0,50):\n",
    "        r_label = random.randint(0, len(X) - 1)\n",
    "        image = X[r_label].reshape(28,28)  #reshape images\n",
    "        i = label_num // 10\n",
    "        j = label_num % 10\n",
    "        axs[i,j].imshow(image) #plot the data\n",
    "        axs[i,j].axis('off')\n",
    "        axs[i,j].set_title(label_dict[y[r_label]])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_images_grid(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to view the \"heatmaps\" for images for one category. The \"heatmaps\" are generalized representations of images coming from one category. \"Heatmaps\" are created out of mean values for pixels for all images from one category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_label_heatmap(X, y, label, label_name):\n",
    "    \"\"\"\n",
    "    Function to plot the heatmap for images with same label.\n",
    "    INPUT:\n",
    "        X - (numpy array) dataset\n",
    "        y - (numpy array) labels for X dataset\n",
    "        label - (int) label for images\n",
    "        label_name - (str) name for images label\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    # filter X_train to remove all other images\n",
    "    label_filter = y == label\n",
    "    X = pd.DataFrame(X)\n",
    "    X_labeled = X[label_filter]\n",
    "\n",
    "    # find mean value for pixels\n",
    "    X_mean = np.sum(X_labeled, axis = 0).values\n",
    "\n",
    "    return X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(10,5))\n",
    "\n",
    "for key, value in label_dict.items():\n",
    "    # get heatmap\n",
    "    heatmap = get_label_heatmap(X, y, key, value)\n",
    "    \n",
    "    i = key // 5\n",
    "    j = key % 5\n",
    "    \n",
    "    # plot image\n",
    "    axs[i,j].set_title(value)\n",
    "    axs[i,j].imshow(heatmap.reshape(28, 28).squeeze())\n",
    "    axs[i,j].axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmaps demonstrated on visualizations above, in fact, represent the generalized idea of each class. Looking at the heatmaps, we can make a lot of interesting observations:\n",
    "* People who play the game give the star a five-pointed representation.\n",
    "* People who play the game represent nail as a metal spike (not as a body part).\n",
    "* Game players generally draw the sword pointed upwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a lot of data, and I can even generate additional data by flipping and rotating the images, I decided to use deep learning approaches to classify drawings.\n",
    "<br>I started with a simple fully connected neural network with two hidden layers built with the PyTorch library.\n",
    "The sizes of the layers are as follows:\n",
    "* Input layer: 784 (for 28 x 28 images),\n",
    "* Hidden layer 1: 128,\n",
    "* Hidden layer 2: 100,\n",
    "* Output layer: 10 (the number of classes).\n",
    "\n",
    "For each hidden layer there is:\n",
    "* ReLU activation function,\n",
    "* Batch normalization.\n",
    "\n",
    "The resulting model has hyperparameters as follows:\n",
    "* Learning rate,\n",
    "* Dropout for hidden layers,\n",
    "* Weight decay (L2 regularization),\n",
    "* Optimizer: Adam or SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile quickdraw.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=784, output_size=10, hidden_sizes=None, dropout=0.0):\n",
    "        super(Net, self).__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [128, 100, 64]\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                          ('relu1', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                          ('bn2', nn.BatchNorm1d(num_features=hidden_sizes[1])),\n",
    "                          ('relu2', nn.ReLU()),\n",
    "                          ('dropout', nn.Dropout(dropout)),\n",
    "                          ('fc3', nn.Linear(hidden_sizes[1], hidden_sizes[2])),\n",
    "                          ('bn3', nn.BatchNorm1d(num_features=hidden_sizes[2])),\n",
    "                          ('relu3', nn.ReLU()),\n",
    "                          ('logits', nn.Linear(hidden_sizes[2], output_size))]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quickdraw import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size, hidden_sizes, dropout = 0.0):\n",
    "    '''\n",
    "    Function creates deep learning model based on parameters passed.\n",
    "\n",
    "    INPUT:\n",
    "        input_size, output_size, hidden_sizes - layer sizes\n",
    "        dropout - dropout (probability of keeping a node)\n",
    "\n",
    "    OUTPUT:\n",
    "        model - deep learning model\n",
    "    '''\n",
    "\n",
    "    # Build a feed-forward network\n",
    "    return Net(input_size, output_size, hidden_sizes, dropout)\n",
    "\n",
    "def shuffle(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Function which shuffles training dataset.\n",
    "    INPUT:\n",
    "        X_train - (tensor) training set\n",
    "        y_train - (tensor) labels for training set\n",
    "\n",
    "    OUTPUT:\n",
    "        X_train_shuffled - (tensor) shuffled training set\n",
    "        y_train_shuffled - (tensor) shuffled labels for training set\n",
    "    \"\"\"\n",
    "    X_train_shuffled = X_train.numpy()\n",
    "    y_train_shuffled = y_train.numpy().reshape((X_train.shape[0], 1))\n",
    "\n",
    "    permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "    X_train_shuffled = X_train_shuffled[permutation, :]\n",
    "    y_train_shuffled = y_train_shuffled[permutation, :].reshape((X_train.shape[0], 1))\n",
    "\n",
    "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
    "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
    "\n",
    "    return X_train_shuffled, y_train_shuffled\n",
    "\n",
    "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD', model_version = ''):\n",
    "    \"\"\"\n",
    "    Function which fits the model.\n",
    "\n",
    "    INPUT:\n",
    "        model - pytorch model to fit\n",
    "        X_train - (tensor) train dataset\n",
    "        y_train - (tensor) train dataset labels\n",
    "        epochs - number of epochs\n",
    "        n_chunks - number of chunks to cplit the dataset\n",
    "        learning_rate - learning rate value\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "    print_every = 10\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            output = model.forward(images[i])\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if epochs % print_every == 0:\n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "\n",
    "            running_loss = 0\n",
    "    torch.save(model.state_dict(), model_version)\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://minio-service.kubeflow:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\",\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    with open(model_version, \"rb\") as fp:\n",
    "        s3.upload_fileobj(fp, \"mlpipeline\", os.path.join(model_version, \"model.pt\"))\n",
    "    with open(\"quickdraw.py\", \"rb\") as fp:\n",
    "        s3.upload_fileobj(fp, \"mlpipeline\", os.path.join(model_version, \"quickdraw.py\"))\n",
    "\n",
    "def view_classify(img, ps):\n",
    "    \"\"\"\n",
    "    Function for viewing an image and it's predicted classes\n",
    "    with matplotlib.\n",
    "\n",
    "    INPUT:\n",
    "        img - (tensor) image file\n",
    "        ps - (tensor) predicted probabilities for each class\n",
    "    \"\"\"\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(['cannon','eye', 'face', 'nail', 'pear','piano','radio','spider','star','sword'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def test_model(model, img):\n",
    "    \"\"\"\n",
    "    Function creates test view of the model's prediction for image.\n",
    "\n",
    "    INPUT:\n",
    "        model - pytorch model\n",
    "        img - (tensor) image from the dataset\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 2D image to 1D vector\n",
    "    img = img.resize_(1, 784)\n",
    "\n",
    "    ps = get_preds(model, img)\n",
    "    view_classify(img.resize_(1, 28, 28), ps)\n",
    "\n",
    "def get_preds(model, input):\n",
    "    \"\"\"\n",
    "    Function to get predicted probabilities from the model for each class.\n",
    "\n",
    "    INPUT:\n",
    "        model - pytorch model\n",
    "        input - (tensor) input vector\n",
    "\n",
    "    OUTPUT:\n",
    "        ps - (tensor) vector of predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(input)\n",
    "    ps = F.softmax(logits, dim=1)\n",
    "    return ps\n",
    "\n",
    "def get_labels(pred):\n",
    "    \"\"\"\n",
    "        Function to get the vector of predicted labels for the images in\n",
    "        the dataset.\n",
    "\n",
    "        INPUT:\n",
    "            pred - (tensor) vector of predictions (probabilities for each class)\n",
    "        OUTPUT:\n",
    "            pred_labels - (numpy) array of predicted classes for each vector\n",
    "    \"\"\"\n",
    "\n",
    "    pred_np = pred.numpy()\n",
    "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
    "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
    "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
    "\n",
    "    return pred_labels\n",
    "\n",
    "def evaluate_model(model, train, y_train, test, y_test):\n",
    "    \"\"\"\n",
    "    Function to print out train and test accuracy of the model.\n",
    "\n",
    "    INPUT:\n",
    "        model - pytorch model\n",
    "        train - (tensor) train dataset\n",
    "        y_train - (numpy) labels for train dataset\n",
    "        test - (tensor) test dataset\n",
    "        y_test - (numpy) labels for test dataset\n",
    "\n",
    "    OUTPUT:\n",
    "        accuracy_train - accuracy on train dataset\n",
    "        accuracy_test - accuracy on test dataset\n",
    "    \"\"\"\n",
    "    train_pred = get_preds(model, train)\n",
    "    train_pred_labels = get_labels(train_pred)\n",
    "\n",
    "    test_pred = get_preds(model, test)\n",
    "    test_pred_labels = get_labels(test_pred)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
    "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
    "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
    "    \n",
    "    vocab = np.unique(y_train).tolist()\n",
    "    cm_train = confusion_matrix(y_test, test_pred_labels, labels=vocab)\n",
    "    cm_test = confusion_matrix(y_test, test_pred_labels, labels=vocab)\n",
    "\n",
    "    return accuracy_train, accuracy_test, cm_train, cm_test\n",
    "\n",
    "def plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, epochs = 10, learning_rate = 0.003, weight_decay = 0.0, dropout = 0.0, n_chunks = 1000, optimizer = 'SGD', model_version = ''):\n",
    "    \"\"\"\n",
    "    Function to plot learning curve depending on the number of epochs.\n",
    "\n",
    "    INPUT:\n",
    "        input_size, output_size, hidden_sizes - model parameters\n",
    "        train - (tensor) train dataset\n",
    "        labels - (tensor) labels for train dataset\n",
    "        y_train - (numpy) labels for train dataset\n",
    "        test - (tensor) test dataset\n",
    "        y_test - (numpy) labels for test dataset\n",
    "        epochs - epochs hyperparameter\n",
    "        learning_rate - learning rate hyperparameter\n",
    "        weight_decay - weight decay (regularization)\n",
    "        dropout - dropout for hidden layer\n",
    "        n_chunks - the number of minibatches to train the model\n",
    "        optimizer - optimizer to be used for training (SGD or Adam)\n",
    "        model_version - the model id\n",
    "\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "\n",
    "    # create model\n",
    "    model = build_model(input_size, output_size, hidden_sizes, dropout = dropout)\n",
    "\n",
    "    # fit model\n",
    "    fit_model(model, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = optimizer, model_version = model_version)\n",
    "    # get accuracy\n",
    "    accuracy_train, accuracy_test, cm_train, cm_test = evaluate_model(model, train, y_train, test, y_test)\n",
    "    \n",
    "    return accuracy_train, accuracy_test, cm_train, cm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "train = torch.from_numpy(X_train).float()\n",
    "labels = torch.from_numpy(y_train).long()\n",
    "test = torch.from_numpy(X_test).float()\n",
    "test_labels = torch.from_numpy(y_test).long()\n",
    "\n",
    "workflow_name = None\n",
    "pod_name = None\n",
    "artifact_location = None\n",
    "\n",
    "# Set hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 100, 64]\n",
    "output_size = 10\n",
    "\n",
    "epochs = 10\n",
    "dropout = 0.0\n",
    "weight_decay = 0.0\n",
    "n_chunks = 700\n",
    "learning_rate = 0.03\n",
    "optimizer = 'SGD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model_version = \"model_version_\" + str(uuid4())\n",
    "\n",
    "# Build model\n",
    "model = build_model(input_size, output_size, hidden_sizes, dropout = dropout)\n",
    "\n",
    "# Fit model\n",
    "train_acc, test_acc, cm_train, cm_test = plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, epochs = epochs, learning_rate = learning_rate, dropout = dropout, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer, model_version = model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log metadata to Kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"metrics\": [{\n",
    "      \"name\": \"accuracy-score\", # The name of the metric. Visualized as the column name in the runs table.\n",
    "      \"numberValue\": test_acc,  # The value of the metric. Must be a numeric value.\n",
    "      \"format\": \"PERCENTAGE\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    }]\n",
    "}\n",
    "with open(\"/mlpipeline-metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.unique(y_train).tolist()\n",
    "data = []\n",
    "for target_index, target_row in enumerate(cm_test):\n",
    "    for predicted_index, count in enumerate(target_row):\n",
    "        data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "df_cm = pd.DataFrame(data, columns=[\"target\", \"predicted\", \"count\"])\n",
    "cm_file = \"confusion_matrix.csv\"\n",
    "with open(cm_file, \"w\") as f:\n",
    "    df_cm.to_csv(f, columns=[\"target\", \"predicted\", \"count\"], header=False, index=False)\n",
    "\n",
    "tar_file = \"confusion_matrix.tar.gz\"\n",
    "with tarfile.open(tar_file, \"w:gz\") as tar:\n",
    "    tar.add(cm_file)\n",
    "\n",
    "if workflow_name and pod_name:\n",
    "    artifact_location = os.path.join(\"artifacts\", workflow_name, pod_name, tar_file)\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://minio-service.kubeflow:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\",\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    with open(tar_file, \"rb\") as fp:\n",
    "        s3.upload_fileobj(fp, \"mlpipeline\", artifact_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if artifact_location:\n",
    "    ui_metadata = {\n",
    "        \"outputs\" : [{\n",
    "          \"type\": \"confusion_matrix\",\n",
    "          \"format\": \"csv\",\n",
    "          \"schema\": [\n",
    "            {\"name\": \"target\", \"type\": \"CATEGORY\"},\n",
    "            {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n",
    "            {\"name\": \"count\", \"type\": \"NUMBER\"},\n",
    "          ],\n",
    "          \"source\": 'minio://mlpipeline/' + artifact_location,\n",
    "          \"labels\": categories,\n",
    "        }]\n",
    "    }\n",
    "    with open(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(ui_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws1 = metadata.Workspace(\n",
    "    # Connect to metadata service in namesapce kubeflow in k8s cluster.\n",
    "    store=metadata.Store(grpc_host=\"metadata-grpc-service.kubeflow\", grpc_port=8080),\n",
    "    name=\"quick_draw\",\n",
    "    description=\"a workspace for training models for quick draw dataset\",\n",
    "    labels={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = metadata.Run(\n",
    "    workspace=ws1,\n",
    "    name=\"run-\" + datetime.utcnow().isoformat(\"T\") ,\n",
    "    description=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec = metadata.Execution(\n",
    "    name = \"execution\" + datetime.utcnow().isoformat(\"T\") ,\n",
    "    workspace=ws1,\n",
    "    run=r,\n",
    "    description=\"execution example\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_set_version = \"v1\"\n",
    "data_set = exec.log_input(\n",
    "        metadata.DataSet(\n",
    "            description=\"Quick Draw simplified dataset (.pickle)\",\n",
    "            name=\"Quick Draw\",\n",
    "            owner=\"anonymous\",\n",
    "            uri=\"gs://artifacts.kubeflow-aifest-2019.appspot.com/quickdraw_dataset\",\n",
    "            version=date_set_version,\n",
    "            query=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exec.log_output(\n",
    "    metadata.Model(\n",
    "            name=\"Neural Network\",\n",
    "            description=\"Simple fully connected neural network with two hidden layers\",\n",
    "            owner=\"anonymous\",\n",
    "            uri=\"minio://mlpipeline/\" + model_version,\n",
    "            model_type=\"neural network\",\n",
    "            training_framework={\n",
    "                \"name\": \"pytorch\",\n",
    "                \"version\": \"v1.3\"\n",
    "            },\n",
    "            hyperparameters={\n",
    "                \"dropout\": dropout,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"batch_size\": n_chunks,\n",
    "                \"epochs\": epochs,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"hidden_sizes\": hidden_sizes,\n",
    "                \"optimizer\": optimizer\n",
    "            },\n",
    "            version=model_version,\n",
    "            labels={}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = exec.log_output(\n",
    "    metadata.Metrics(\n",
    "            name=\"Evaluation\",\n",
    "            description=\"\",\n",
    "            owner=\"anonymous\",\n",
    "            uri=\"\",\n",
    "            data_set_id=str(data_set.id),\n",
    "            model_id=str(model.id),\n",
    "            metrics_type=metadata.Metrics.VALIDATION,\n",
    "            values={\"accuracy\": test_acc},\n",
    "            labels={}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel I demonstrated an example how to build simple fully connected neural network to classify simple images. While building the solution we used the original data and artificially generated data and introduced regularization techniques such as dropout to reduce variance of the model. The final model has the accuracy about 80% (this can be improved by adding more data and using more epochs while training, I didn't do that to reduce the running time of the kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "The model performs quite well on ten image classes from the simplified dataset, but there is a lot to improve:\n",
    "* Add more drawing classes;\n",
    "* Try other architectures, such as convolutional neural networks;\n",
    "* Try the full dataset, which contains images with higher resolution and additional information (country, strokes and so on)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
